{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64747c5",
   "metadata": {},
   "source": [
    "# ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b93196",
   "metadata": {},
   "source": [
    "### ãƒã‚¸ãƒã‚¬åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f20f727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import oseti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c1a4c9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'line_data/ã€‡ã€‡ã¨ã®ãƒˆãƒ¼ã‚¯.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5409a5ea449c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# open file and load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mlog_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'line_data/ã€‡ã€‡ã¨ã®ãƒˆãƒ¼ã‚¯.txt'"
     ]
    }
   ],
   "source": [
    "'''\n",
    " ãƒˆãƒ¼ã‚¯å±¥æ­´ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å¤‰æ›\n",
    "'''\n",
    "import re\n",
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Data():\n",
    "    # flag\n",
    "    # 0 : talk meassge\n",
    "    # 10 : call\n",
    "    # 11 : missed call\n",
    "    # 12 : canceled call\n",
    "    # 13 : no answer call\n",
    "    # 2 : photo\n",
    "    # 3 : video\n",
    "    # 4 : sticker\n",
    "    # 50 : system message unsent\n",
    "    # 60 : file\n",
    "    # 70 : create and add album\n",
    "    # 71 : changed the name of the album\n",
    "    # 72 : deleted the album\n",
    "    def __init__(self, year, month, day, hour, minute, person, payload, flag):\n",
    "        self.year = year\n",
    "        self.month = month\n",
    "        self.day = day\n",
    "        self.hour = hour\n",
    "        self.minute = minute\n",
    "        self.person = person\n",
    "        self.payload = payload\n",
    "        self.flag = flag\n",
    "\n",
    "\n",
    "# disable #print\n",
    "# sys.stdout = open(os.devnull, 'w', encoding=\"utf-8\")\n",
    "\n",
    "file_path = \"line_data/ã€‡ã€‡ã¨ã®ãƒˆãƒ¼ã‚¯.txt\"\n",
    "\n",
    "date_ = datetime.datetime.now()\n",
    "logs = []\n",
    "\n",
    "# open file and load data\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "    log_text = f.read()\n",
    "\n",
    "date_pattern = r\"20\\d{2}/\\d{2}/\\d{2}\"\n",
    "message_pattern = r\"\\d{2}:\\d{2}\\t.*\\t.*\"\n",
    "photo_pattern = r\"\\d{2}:\\d{2}\\t.*\\t\\[å†™çœŸ]\"\n",
    "sticker_pattern = r\"\\d{2}:\\d{2}\\t.*\\t\\[ã‚¹ã‚¿ãƒ³ãƒ—]\"\n",
    "video_pattern = r\"\\d{2}:\\d{2}\\t.*\\t\\[å‹•ç”»]\"\n",
    "file_pattern = r\"\\d{2}:\\d{2}\\t.*\\t\\[File]\"\n",
    "album_build_pattern = r\"\\d{2}:\\d{2}\\t.*\\t\\[Albums].*\"\n",
    "album_rename_pattern = r\"\\d{2}:\\d{2}\\t.* changed the name of the album.*\"\n",
    "album_delete_pattern = r\"\\d{2}:\\d{2}\\t.* delete the album.*\"\n",
    "missed_call_pattern = r\"\\d{2}:\\d{2}\\t.*\\tâ˜ ä¸åœ¨ç€ä¿¡\"\n",
    "canceled_call_pattern = r\"\\d{2}:\\d{2}\\t.*\\tâ˜ é€šè©±ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ\"\n",
    "no_answer_call_pattern = r\"\\d{2}:\\d{2}\\t.*\\tâ˜ é€šè©±ã«å¿œç­”ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ\"\n",
    "call_pattern = r\"\\d{2}:\\d{2}\\t.*\\tâ˜ é€šè©±æ™‚é–“ (\\d{1,2}:\\d{2}|\\d{1,2}:\\d{2}:\\d{2})\"\n",
    "sys_unsent_pattern = r\"\\d{2}:\\d{2}\\t.* é€ä¿¡å–ã‚Šæ¶ˆã—\"\n",
    "\n",
    "for i, log in enumerate(log_text.splitlines()[3:]):\n",
    "    #print(f\"{log} : \", end='')\n",
    "    if log == '':\n",
    "        #print(\"no data\")\n",
    "        continue\n",
    "    date_stamp = \"\"\n",
    "    if re.match(date_pattern, log):\n",
    "        #print(\"day data\")\n",
    "        date_stamp = log.replace('/', ',').replace(' ', ',')[0:10]\n",
    "        date_ = datetime.datetime.strptime(date_stamp, '%Y,%m,%d')\n",
    "    elif re.match(photo_pattern, log):\n",
    "        #print(\"photo data\")\n",
    "        splited_log = re.split('\\t', log)\n",
    "        logs.append(Data(date_.year, date_.month, date_.day,\n",
    "                         splited_log[0][0:2], splited_log[0][3:5], splited_log[1], \"\", 2))\n",
    "    elif re.match(video_pattern, log):\n",
    "        #print(\"Video data\")\n",
    "        splited_log = re.split('\\t', log)\n",
    "        logs.append(Data(date_.year, date_.month, date_.day,\n",
    "                         splited_log[0][0:2], splited_log[0][3:5], splited_log[1], \"\", 3))\n",
    "    elif re.match(sticker_pattern, log):\n",
    "        #print(\"Sticker data\")\n",
    "        splited_log = re.split('\\t', log)\n",
    "        logs.append(Data(date_.year, date_.month, date_.day,\n",
    "                         splited_log[0][0:2], splited_log[0][3:5], splited_log[1], \"\", 4))\n",
    "    elif re.match(call_pattern, log):\n",
    "        #print(\"call data\")\n",
    "        splited_log = re.split('\\t', log)\n",
    "        time_data = splited_log[2][7:]\n",
    "        time_data = re.split(':', time_data)\n",
    "        time_length = 0\n",
    "        for i in range(len(time_data)):\n",
    "            time_length += int(time_data[len(time_data) - i - 1]) * (60 ** i)\n",
    "        # print(time_length)\n",
    "        logs.append(Data(date_.year, date_.month, date_.day,\n",
    "                         splited_log[0][0:2], splited_log[0][3:5], splited_log[1], time_length, 10))\n",
    "    elif re.match(missed_call_pattern, log):\n",
    "        #print(\"Missed call data\")\n",
    "        splited_log = re.split('\\t', log)\n",
    "        logs.append(Data(date_.year, date_.month, date_.day,\n",
    "                         splited_log[0][0:2], splited_log[0][3:5], splited_log[1], \"\", 11))\n",
    "    elif re.match(canceled_call_pattern, log):\n",
    "        #print(\"Canceled call data\")\n",
    "        splited_log = re.split('\\t', log)\n",
    "        logs.append(Data(date_.year, date_.month, date_.day,\n",
    "                         splited_log[0][0:2], splited_log[0][3:5], splited_log[1], \"\", 12))\n",
    "    elif re.match(no_answer_call_pattern, log):\n",
    "        #print(\"no answer call data\")\n",
    "        splited_log = re.split('\\t', log)\n",
    "        logs.append(Data(date_.year, date_.month, date_.day,\n",
    "                         splited_log[0][0:2], splited_log[0][3:5], splited_log[1], \"\", 13))\n",
    "    elif re.match(sys_unsent_pattern, log):\n",
    "        #print(\"sys unsent data\")\n",
    "        splited_log = re.split('\\t', log)\n",
    "        logs.append(Data(date_.year, date_.month, date_.day,\n",
    "                         splited_log[0][0:2], splited_log[0][3:5], \"\", \"\", 50))\n",
    "    elif re.match(file_pattern, log):\n",
    "        #print(\"file data\")\n",
    "        splited_log = re.split('\\t', log)\n",
    "        logs.append(Data(date_.year, date_.month, date_.day,\n",
    "                         splited_log[0][0:2], splited_log[0][3:5], \"\", \"\", 60))\n",
    "    elif re.match(album_build_pattern, log):\n",
    "        #print(\"create album data\")\n",
    "        splited_log = re.split('\\t', log)\n",
    "        logs.append(Data(date_.year, date_.month, date_.day,\n",
    "                         splited_log[0][0:2], splited_log[0][3:5], \"\", \"\", 70))\n",
    "    elif re.match(album_rename_pattern, log):\n",
    "        #print(\"rename album data\")\n",
    "        splited_log = re.split('\\t', log)\n",
    "        logs.append(Data(date_.year, date_.month, date_.day,\n",
    "                         splited_log[0][0:2], splited_log[0][3:5], \"\", \"\",71))\n",
    "    elif re.match(album_delete_pattern, log):\n",
    "        #print(\"delete album data\")\n",
    "        splited_log = re.split('\\t', log)\n",
    "        logs.append(Data(date_.year, date_.month, date_.day,\n",
    "                         splited_log[0][0:2], splited_log[0][3:5], \"\", \"\", 72))\n",
    "    elif re.match(message_pattern, log):\n",
    "        #print(\"message data\")\n",
    "        splited_log = re.split('\\t', log)\n",
    "        logs.append(Data(date_.year, date_.month, date_.day,\n",
    "                         splited_log[0][0:2], splited_log[0][3:5], splited_log[1], splited_log[2], 0))\n",
    "    elif (len(re.split('\\t', log)) == 1):\n",
    "        splited_log = re.split('\\t', log)\n",
    "        #print(\"returned data\")\n",
    "        if len(logs)!=0:\n",
    "            logs[-1].payload += log\n",
    "    else:\n",
    "        pass\n",
    "        #print(\"\\nNo classified data\\n\")\n",
    "\n",
    "with open('tempo_data/line.csv', 'w', encoding=\"utf-8\", newline=\"\") as f:\n",
    "    for content in logs:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([str(content.year), str(content.month), str(content.day), str(content.hour),str(content.minute), str(content.person), str(content.payload), str(content.flag)])\n",
    "\n",
    "print(\"SuccessğŸ‰\")\n",
    "\n",
    "\"\"\"\n",
    " ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«æ ¼ç´\n",
    "\"\"\"\n",
    "df = pd.read_csv(\"tempo_data/line.csv\", names=('year', 'month', 'day',\n",
    "                                   'hour', 'minute', 'person', 'payloads', 'flag'), encoding=\"UTF-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3e9b8b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-58683505073a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moseti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAnalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#ã‚³ãƒ¡ãƒ³ãƒˆã”ã¨ã«ãƒã‚¸ãƒã‚¬åˆ¤å®š\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"posi_nega\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"payloads\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"posi_nega\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"posi_nega\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "###-----------------------------------------------------------------------\n",
    "### ãƒã‚¸ãƒã‚¬åˆ¤å®š\n",
    "###-----------------------------------------------------------------------\n",
    "analyzer = oseti.Analyzer()\n",
    "#ã‚³ãƒ¡ãƒ³ãƒˆã”ã¨ã«ãƒã‚¸ãƒã‚¬åˆ¤å®š\n",
    "df[\"posi_nega\"] = df[\"payloads\"].apply(lambda x: analyzer.analyze(str(x)))\n",
    "df[\"posi_nega\"] = df[\"posi_nega\"].apply(lambda x: sum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2a5dde9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3502f1a7552c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m###-----------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#ãƒã‚¸ãƒã‚¬ã‚¹ã‚³ã‚¢ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_groupby_posinega\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'posi_nega'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å‡¦ç†\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf_groupby_posinega\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_groupby_posinega\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "###-----------------------------------------------------------------------\n",
    "### ã‚¹ã‚³ã‚¢è¨ˆç®—ã®è«¸ã€…ã®å‡¦ç†\n",
    "###-----------------------------------------------------------------------\n",
    "#ãƒã‚¸ãƒã‚¬ã‚¹ã‚³ã‚¢ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–\n",
    "df_groupby_posinega =  df.groupby('posi_nega').count()\n",
    "#ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å‡¦ç†\n",
    "df_groupby_posinega = df_groupby_posinega.reset_index()\n",
    "#é‡ã¿ä»˜ã‘ã—ãŸãƒã‚¸ãƒã‚¬ã‚¹ã‚³ã‚¢ã®ç®—å‡º\n",
    "df_groupby_posinega['score'] = 0\n",
    "df_groupby_posinega['score'] = df_groupby_posinega.apply(lambda x: x['year'] * x['posi_nega'], axis = 1)\n",
    "#ã‚¹ã‚³ã‚¢ã‚’æ ¼ç´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ df_posinegaã®ä½œæˆ\n",
    "df_posinega = pd.DataFrame(np.arange(3).reshape(1, 3),\n",
    "                          columns = [\"positive\", \"negative\", \"neutral\"],\n",
    "                          index = [\"score\"])\n",
    "\n",
    "df_posinega[\"positive\"] = df_groupby_posinega[\"score\"].where(df_groupby_posinega[\"score\"] > 0).sum()\n",
    "df_posinega[\"negative\"] = -(df_groupby_posinega[\"score\"].where(df_groupby_posinega[\"score\"] < 0).sum())\n",
    "df_posinega[\"neutral\"] = df_groupby_posinega[\"score\"].where(df_groupby_posinega[\"score\"] == 0).count()\n",
    "\n",
    "\n",
    "\n",
    "###-----------------------------------------------------------------------\n",
    "### ã‚°ãƒ©ãƒ•ã®ä½œæˆ\n",
    "###-----------------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = [df_posinega[\"positive\"].item(), df_posinega[\"negative\"].item(), df_posinega[\"neutral\"].item()]\n",
    "labels = [\"positive\", \"negative\", \"neutral\"]#å††ã‚°ãƒ©ãƒ•ç”¨ãƒ©ãƒ™ãƒ«\n",
    "\n",
    "cmap = plt.get_cmap(\"Paired\")\n",
    "colors = [cmap(i) for i in range(len(x))]\n",
    "\n",
    "wedgeprops={\"edgecolor\":\"white\", \"width\":0.6}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(x, labels=labels,\n",
    "      startangle=90,\n",
    "      counterclock = False,\n",
    "      colors = colors,\n",
    "      wedgeprops = wedgeprops)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e6febca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c95976be0197>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m### ãƒã‚¬ãƒ†ã‚£ãƒ–æ–‡ç« ã®æŠœãå‡ºã—\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m###-------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_negative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'posi_nega'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf_negative\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"person\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m###æ°—ã«ãªã‚‹å±æ€§ã§ã‚°ãƒ«ãƒ¼ãƒ—ãƒã‚¤ã‚’ã—ã¦ã„ã‘ã°ã„ã„ã‚ˆã­\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "###-------------------------------------------------------\n",
    "### ãƒã‚¬ãƒ†ã‚£ãƒ–æ–‡ç« ã®æŠœãå‡ºã—\n",
    "###-------------------------------------------------------\n",
    "df_negative = df[df['posi_nega'] < 0]\n",
    "df_negative.groupby(\"person\").count()\n",
    "###æ°—ã«ãªã‚‹å±æ€§ã§ã‚°ãƒ«ãƒ¼ãƒ—ãƒã‚¤ã‚’ã—ã¦ã„ã‘ã°ã„ã„ã‚ˆã­\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfb77fc",
   "metadata": {},
   "source": [
    "### ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ba9717a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e3fbc00bef36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m### å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m###---------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "###---------------------------------------------------------------\n",
    "### å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "###---------------------------------------------------------------\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import MeCab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "###---------------------------------------------------------------\n",
    "###  ã‚³ãƒ¡ãƒ³ãƒˆã®ã¿ã‚’æ–‡ç« ã¨ã—ã¦æŠ½å‡º\n",
    "###---------------------------------------------------------------\n",
    "df = pd.read_csv(\"tempo_data/line.csv\", names=('year', 'month', 'day',\n",
    "                                   'hour', 'minute', 'person', 'payloads', 'flag'), encoding=\"UTF-8\")\n",
    "df_comment = df[\"payloads\"]\n",
    "df_comment.to_csv(\"tempo_data/LINEãƒˆãƒ¼ã‚¯æŠ½å‡º.csv\",\n",
    "                  header=None,\n",
    "                  index=False,\n",
    "                  encoding=\"UTF-8\")\n",
    "\n",
    "\n",
    "text_file = open(\"tempo_data/LINEãƒˆãƒ¼ã‚¯æŠ½å‡º.csv\")\n",
    "full_text = text_file.read()\n",
    "full_text= full_text.replace(\"\\n\",\"\")\n",
    "\n",
    "#print(full_text)  \n",
    "\n",
    "###--------------------------------------------------------------\n",
    "### MeCabã§å½¢æ…‹ç´ è§£æ\n",
    "###--------------------------------------------------------------\n",
    "\n",
    "fpath = '/Users/tomoriyuuki/Library/Fonts/NotoSansCJKjp-Bold.otf'  # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆæŒ‡å®š\n",
    "stop_words_ja = ['ã‚‚ã®', 'ã“ã¨', 'ã¨ã', 'ãã†', 'ãŸã¡', 'ã“ã‚Œ', 'ã‚ˆã†', 'ã“ã‚Œã‚‰', 'ãã‚Œ', 'ã™ã¹ã¦', 'ç¬‘ç¬‘', 'ç¬‘', 'ä½•', 'ã®', 'ã‚“', 'äºº']#çœããŸã„è¨€è‘‰\n",
    "tagger = MeCab.Tagger(\"-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "#tagger = MeCab.Tagger()\n",
    "tagger.parse('')\n",
    "node = tagger.parseToNode(full_text)\n",
    "\n",
    "word_list = []\n",
    "while node:\n",
    "        word_type = node.feature.split(',')[0]\n",
    "        word_surf = node.surface.split(',')[0]\n",
    "        if word_type == 'åè©' and word_surf not in stop_words_ja:\n",
    "            word_list.append(node.surface)\n",
    "        node = node.next\n",
    "\n",
    "word_chain = ' '.join(word_list)\n",
    "wordcloud = WordCloud(background_color=\"white\",\n",
    "                          font_path=fpath,\n",
    "                          width=900,\n",
    "                          height=500,\n",
    "                          contour_width=1,\n",
    "                          contour_color=\"black\",\n",
    "                          stopwords=set(stop_words_ja)).generate(word_chain)\n",
    "\n",
    "#æç”»\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "#pngå‡ºåŠ›\n",
    "wordcloud.to_file(\"img_data/wc_image.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58795048",
   "metadata": {},
   "source": [
    "### å¥½ããªå½¢ã«ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e6daacd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4ef37c443073>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseToNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m#maskå–å¾—\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mask_data/apple.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'full_text' is not defined"
     ]
    }
   ],
   "source": [
    "def get_mask(img_path):\n",
    "    import cv2\n",
    "    img = cv2.imread(img_path, -1)\n",
    "    a_img = img[:, :, 3]\n",
    "    result_img = cv2.bitwise_not(a_img)\n",
    " \n",
    "    return result_img\n",
    "\n",
    "fpath = '/Users/tomoriyuuki/Library/Fonts/NotoSansCJKjp-Bold.otf'  # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆæŒ‡å®š\n",
    "stop_words_ja =  ['ã‚‚ã®', 'ã“ã¨', 'ã¨ã', 'ãã†', 'ãŸã¡', 'ã“ã‚Œ', 'ã‚ˆã†', 'ã“ã‚Œã‚‰', 'ãã‚Œ', 'ã™ã¹ã¦', 'ç¬‘ç¬‘', 'ç¬‘', 'ä½•', 'ã®', 'ã‚“', 'äºº']#çœããŸã„è¨€è‘‰\n",
    "tagger = MeCab.Tagger(\"-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "tagger.parse('')\n",
    "node = tagger.parseToNode(full_text)\n",
    "#maskå–å¾—\n",
    "mask = get_mask(\"mask_data/apple.png\")\n",
    "word_list = []\n",
    "while node:\n",
    "        word_type = node.feature.split(',')[0]\n",
    "        word_surf = node.surface.split(',')[0]\n",
    "        if word_type == 'åè©' and word_surf not in stop_words_ja:\n",
    "            word_list.append(node.surface)\n",
    "        node = node.next\n",
    "\n",
    "word_chain = ' '.join(word_list)\n",
    "wordcloud = WordCloud(background_color=\"White\",\n",
    "                          font_path=fpath,\n",
    "                          contour_width=1,\n",
    "                          mask = mask,\n",
    "                          max_font_size = 100,\n",
    "                          max_words = 1000,\n",
    "                          #mode = \"RGBA\",\n",
    "                          #colormap=\"hsv\",\n",
    "                          stopwords=set(stop_words_ja)).generate(word_chain)\n",
    "\n",
    "#æç”»\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "#pngå‡ºåŠ›\n",
    "wordcloud.to_file(\"img_data/wc_mask_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95cf1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
